{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a34240cc-aa83-4e01-ad78-8768d9b78750",
   "metadata": {},
   "source": [
    "# Fine-tuning Example\n",
    "\n",
    "This notebook demonstrates how to fine-tune a code-generating LLM to observe a specific prompt and output format.\n",
    "We want Python functions to be enclosed in XML tags:\n",
    "\n",
    "```python\n",
    "# <function name=\"foo\">\n",
    "def foo():\n",
    "    # content\n",
    "# </function>\n",
    "```\n",
    "\n",
    "We will obtain training data by extracting functions from a repository using GitPython and tree-sitter.\n",
    "\n",
    "## ⚠️ Warnings\n",
    "\n",
    "⚡ Fine-tuning happens in-place, which means the **original model is lost** once the PEFT-Adapter is wrapped around it. If you need to test the original model again or re-start with different parameters, **restart the notebook kernel**.\n",
    "\n",
    "⚡ If your loss tensor reports **NaN**, the training did not converge. Try reducing learning rate, introduce warm-up, change the order of training data, or other parameters (like LoRA rank or alpha). Make sure your model is loaded in **bfloat16**, because float16 does not always have sufficient range to cover all optimization steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83284a93-f14e-4d92-8bff-fa6db4c993aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "from transformers import GemmaTokenizer, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32325982-1146-4e53-97e7-8942e2139230",
   "metadata": {},
   "source": [
    "# Load an LLM\n",
    "* This example uses [CodeGemma 1.1](https://huggingface.co/google/codegemma-1.1-2b)\n",
    "* We use the small 2B variant, note that fine-tuning larger models is much more memory-intensive!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3806a2f6-572d-44a5-87e4-d4c48d65a71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e180f69e7c034238b41e44af3a03dbed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "gpu = torch.device('cuda:0')\n",
    "model_id = \"google/codegemma-1.1-2b\"\n",
    "tokenizer = GemmaTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=gpu, torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b70d0ebf-5287-4e92-b2af-14144fa43cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A standard greedy generation helper\n",
    "def generate(prompt, max_new_tokens=200):\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt').to(gpu)\n",
    "    outputs = model.generate(inputs, max_new_tokens=max_new_tokens)\n",
    "    return tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "debfc182-4ef7-4038-bdd5-1bef0a8b66b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos># <function name=\"test_http_404\">\n",
      "def test_http_404():\n",
      "    \"\"\"\n",
      "    Test the HTTP 404 response for a non-existent page.\n",
      "    \"\"\"\n",
      "    response = client.get('/non-existent-page')\n",
      "    assert response.status_code == 404\n",
      "    assert response.content == b'<h1>404 Not Found</h1>'\n",
      "    assert response.headers['Content-Type'] == 'text/html; charset=utf-8'\n",
      "    assert response.headers['Cache-Control'] == 'no-cache, no-store, must-revalidate'\n",
      "    assert response.headers['Pragma'] == 'no-cache'\n",
      "    assert response.headers['Expires'] == '0'\n",
      "    assert response.headers['X-Content-Type-Options'] == 'nosniff'\n",
      "    assert response.headers['X-Frame-Options'] == 'DENY'\n",
      "    assert response.headers['X-\n"
     ]
    }
   ],
   "source": [
    "# Test how the model responds to the desired prompt (probably not so well)\n",
    "print(generate('# <function name=\"test_http_404\">\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccc1608-4055-4480-81a9-57bf78f70b52",
   "metadata": {},
   "source": [
    "# Data Procurement\n",
    "\n",
    "## Getting Raw Data\n",
    "This part requires the packages `autopep8`, `tree-sitter-python`, and `GitPython`.\n",
    "\n",
    "* We will parse all `.py` files from the current commit in the [Flask](https://github.com/pallets/flask) repository\n",
    "* Using a tree-sitter query, we extract all function definitions\n",
    "* We wrap them in the desired prompt format with XML tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf3ec871-93c5-486b-a511-0828af25327c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import git\n",
    "import autopep8\n",
    "import tree_sitter_python as tspython\n",
    "from tree_sitter import Language, Parser\n",
    "\n",
    "PY_LANGUAGE = Language(tspython.language(), \"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "609a897b-ae0c-4d8d-9a67-1f703b3bdd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = git.Repo('./flask.git')\n",
    "tree = repo.head.commit.tree\n",
    "\n",
    "parser = Parser()\n",
    "parser.set_language(PY_LANGUAGE)\n",
    "query = PY_LANGUAGE.query('''(function_definition) @func''')\n",
    "\n",
    "files = [item.data_stream.read()\n",
    "         for item in tree.list_traverse()\n",
    "         if item.type == 'blob'\n",
    "         and item.name.endswith('.py')]\n",
    "\n",
    "def format_node(node):\n",
    "    code = autopep8.fix_code(node.text.decode('utf-8'))\n",
    "    name = node.child_by_field_name('name').text.decode('utf-8')\n",
    "    return f'# <function name=\"{name}\">\\n{code}# </function>'\n",
    "\n",
    "functions = [format_node(node)\n",
    "             for file in files\n",
    "             for node, _ in query.captures(parser.parse(file).root_node)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6125f6b2-db16-4d9b-a6b3-e5b9b8485e41",
   "metadata": {},
   "source": [
    "## Format Data for Training\n",
    "\n",
    "Next, we need to process the dataset into a format consumable by an LLM training procedure.\n",
    "* We demonstrate the use of the `datasets` library to deal with (possibly large) datasets\n",
    "* We **tokenize** our training examples\n",
    "* To evaluate whether training improves something, we split of a small **test set**, the remaining data is our **train set**\n",
    "* Training happens in **blocks of the same size**, so we split our training data into blocks of equal token numbers, possibly **padding** the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54b758f7-6bb3-4194-8ad5-a0aad7ec5b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "816c9b02-740f-4477-adee-1c2b69d23fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.Dataset.from_dict({'source': functions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1bb2fd0-e46c-447a-beb2-566be8f4829d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# <function name=\"test_multi_route_class_views\">\n",
      "def test_multi_route_class_views(app, client):\n",
      "    class View:\n",
      "        def __init__(self, app):\n",
      "            app.add_url_rule(\"/\", \"index\", self.index)\n",
      "            app.add_url_rule(\"/<test>/\", \"index\", self.index)\n",
      "\n",
      "        def index(self, test=\"a\"):\n",
      "            return test\n",
      "\n",
      "    _ = View(app)\n",
      "    rv = client.open(\"/\")\n",
      "    assert rv.data == b\"a\"\n",
      "    rv = client.open(\"/b/\")\n",
      "    assert rv.data == b\"b\"\n",
      "# </function>\n"
     ]
    }
   ],
   "source": [
    "print(data[300]['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "979059d6-eb46-47b2-84cf-f521b59676b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(dataset_row):\n",
    "    source = dataset_row['source']\n",
    "    input_ids = tokenizer.encode(source) + [tokenizer.eos_token_id]\n",
    "    labels = input_ids.copy()\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'labels': labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "489152d4-d28f-4a96-8e62-d2015b320960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fed3d1c50904cc3b4713d8fb4332145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1406 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_data = data.map(tokenize, remove_columns=['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b0f20ae-c05d-4ed0-9c4e-ce203c224da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def block(data, block_size=128):\n",
    "    '''Arranges a batch into blocks of given token number'''\n",
    "\n",
    "    # concatenate all items\n",
    "    concatenated = sum(data['input_ids'], [])\n",
    "    length = len(concatenated)\n",
    "\n",
    "    # shape \"n / block_size\" blocks\n",
    "    truncated_length = (length // block_size) * block_size\n",
    "    blocked_ids = [concatenated[i : i + block_size] for i in range(0, truncated_length, block_size)]\n",
    "\n",
    "    # add last block with padding\n",
    "    pad_length = block_size - (length % block_size)  # remaining tokens to fill\n",
    "    if pad_length != block_size:\n",
    "        blocked_ids += [concatenated[truncated_length:] + [tokenizer.eos_token_id] * pad_length]\n",
    "\n",
    "    # format as transformers-friendly model input\n",
    "    assert len(blocked_ids) > 0\n",
    "    return {\n",
    "        'input_ids': blocked_ids,\n",
    "        'labels': blocked_ids.copy()}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b78efe2-9869-4003-a1fa-77f64544852c",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset = tokenized_data.train_test_split(\n",
    "    test_size = 0.1,\n",
    "    shuffle = True,\n",
    "    seed = 421337)\n",
    "test_data = split_dataset['test']\n",
    "train_data = split_dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40742267-fc6d-43c3-92aa-a68e64f129c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97ca865d2bb0467583ff07187d71a581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/141 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9b0aa32029646eba105083714de8b87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1265 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_data_blocks = test_data.map(block, batched=True)\n",
    "train_data_blocks = train_data.map(block, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c75087-263e-43f3-8bb6-ec53d8584d77",
   "metadata": {},
   "source": [
    "# Fine-tuning using LoRA\n",
    "\n",
    "## Configuring the Training Procedure\n",
    "\n",
    "To perform training, we have to decide on a method, which will be LoRA in our case.\n",
    "* We configure a **LoRA adapter** which insertes the additional matrices into the model\n",
    "* We configure a **Collator** which takes care of correctly aligning our data in (GPU) memory\n",
    "* We configure a **DataLoader** which creates training batches from our collated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61f38b39-3486-4ab9-9f93-21a9cad39534",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "from transformers import get_linear_schedule_with_warmup, DataCollatorForLanguageModeling\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0499b9a8-253e-4de8-8030-fe5eca218200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm()\n",
       "        (post_attention_layernorm): GemmaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check all model layers. We are looking for the names of the attention matrices (Q and V)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f5a096-94d0-499c-a376-5b79f397aa08",
   "metadata": {},
   "source": [
    "### The LoRA Parameters\n",
    "For this experiment, we will \"factorize\" each matrix into rank 8 (`r = 8`), which means the diff to each m * n matrix is represented as the product of an n * 8 and an 8 * m matrix.\n",
    "We can also configure `alpha`, which is the degree to which the diff overrides the original matrix.\n",
    "**Note that `r` and `alpha` are first guesses. Depending on your task you may need to adjust them and see if performance improves!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e745c2d-3136-40b2-ae19-68eec2ecd9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, \n",
    "    inference_mode=False,\n",
    "    target_modules=['q_proj', 'v_proj'],   # this is specific to each model! Look up the exact names in the model\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,  # to help with generalization, 5% of updates to the LoRA weights are discarded\n",
    "    bias=\"all\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4634d0b5-bbcc-4554-a33e-f9410671ec3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This wraps our LLM into the adapter model\n",
    "peft_model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0f70669-8bcd-47e6-8cc1-999b12963d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 921,600 || all params: 2,507,094,016 || trainable%: 0.036759690467068624\n"
     ]
    }
   ],
   "source": [
    "# Check how many paramaters we need to train (should be orders of magnitude fewer)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb6fd8f-b4ec-428c-b197-401908a54284",
   "metadata": {},
   "source": [
    "### Formatting for Training\n",
    "\n",
    "* We need to decide on a **batch size**, the number of blocks we show and train in parallel each step. To save GPU memory, we stick to a low number. If you have space, increase that value.\n",
    "* We decide on a number of training **epochs**, how often we show all data to the model. As long as we see improvements, we can increase the number. For fine-tuning, a small number (1 - 3) is often okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f2f7812-009e-4ab6-84f8-ed4e88c4c760",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "816885de-f4fd-4b04-9964-fd26449397b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some book-keeping to make sure any padding happens with <EOS> tokens (not all tokenizers are meant to be used in fine-tuning and don't always have this right)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# special collator that takes care of correctly offsetting our data (so that token n predicts n+1) \n",
    "collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer,\n",
    "            mlm=False,  # we could also \"mask\" random tokens to introduce some noise, but we don't need that here\n",
    "            pad_to_multiple_of=8,\n",
    "            return_tensors=\"pt\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "379c5497-f0eb-4eb3-96a8-c7710f17b736",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data_blocks, collate_fn=collator, batch_size=batch_size)\n",
    "eval_dataloader = DataLoader(test_data_blocks, collate_fn=collator, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384dc586-e484-4e7f-b5b8-8bddc7430d54",
   "metadata": {},
   "source": [
    "### Configuring the Optimizer\n",
    "\n",
    "The optimizer is the core of our training. It updates all parameters according to their gradient and the loss they incurred.\n",
    "* We need to set a **learning rate**, which is the proportion of the gradient that gets added each step. This value tends to be low and needs experimentation to set right. We guess a value between 1/1000 and 1/10000 to start with.\n",
    "* The learning rate is changed over time by a **scheduler**. This ensures we \"converge\" over time by learning in smaller and smaller steps.\n",
    "* We could configure a **warmup** in which learning rate increases before it decreases again to make sure the model can \"settle\" a bit before full updates happen. We don't do this here, but if the training data is extremely different than what the model has been pre-trained on, we can \"stabilize\" training this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aeac76a4-b718-4bfe-a32a-12010d5e4e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-4\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)  # The AdamW optimizer is well-suited for LLMs\n",
    "\n",
    "# our schedule will decrease learning rate linearly with time\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=(len(train_dataloader) * num_epochs),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cf0740-a0de-4901-9115-a74f52e6ae69",
   "metadata": {},
   "source": [
    "## The Training Loop\n",
    "\n",
    "Here, we rely on all the bits we configured above:\n",
    "* We set the model to **training mode**\n",
    "* Our **DataLoader** yields training batches\n",
    "* We **run** the batches through the model, the model remembers its gradients since it is in training mode and we obtain a **loss**\n",
    "* We **back-propagate the loss** through the model. Now each weight knows how much it contributed to the error in the model output.\n",
    "* The **Optimizer** updates the weights based on this information\n",
    "* The **Scheduler** updates the optimizer's learning rate\n",
    "* We repeat the above for every batch in the training set\n",
    "* To check whether we converge on the test data, we run each test batch through the model and average the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4c8e1da-e1e3-4b64-b9b9-32e7aa801483",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 301/301 [00:16<00:00, 18.09it/s]\n",
      "100%|██████████| 27/27 [00:00<00:00, 39.64it/s]\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0: train_epoch_loss=tensor(1.4998) eval_epoch_loss=tensor(1.2637)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 301/301 [00:16<00:00, 18.15it/s]\n",
      "100%|██████████| 27/27 [00:00<00:00, 39.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1: train_epoch_loss=tensor(1.1866) eval_epoch_loss=tensor(1.2272)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()  # set to training mode\n",
    "    total_loss = 0\n",
    "    \n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        outputs = model(**batch.to(gpu))  # run batch through model\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.detach().cpu().float()\n",
    "        loss.backward()      # propagate loss back\n",
    "        optimizer.step()     # update weights\n",
    "        lr_scheduler.step()  # update learning rate\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    model.eval()  # set to evaluation mode\n",
    "    eval_loss = 0\n",
    "    for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch.to(gpu))\n",
    "        loss = outputs.loss\n",
    "        eval_loss += loss.detach().cpu().float()\n",
    "\n",
    "    eval_epoch_loss = eval_loss / len(eval_dataloader)\n",
    "    train_epoch_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"{epoch=}: {train_epoch_loss=} {eval_epoch_loss=}\")\n",
    "\n",
    "    # save adapter to be loaded later\n",
    "    peft_model.save_pretrained(f'./my-checkpoint-ep{epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8817742-aeb2-4938-8ec1-c4a647a9d740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos># <function name=\"test_http_404\">\n",
      "def test_http_404(app):\n",
      "    @app.route(\"/notfound\")\n",
      "    def notfound():\n",
      "        return \"not found\", 404\n",
      "\n",
      "    rv = app.test_client().get(\"/notfound\")\n",
      "    assert rv.status_code == 404\n",
      "    assert rv.data == b\"not found\"\n",
      "# </function><eos>\n"
     ]
    }
   ],
   "source": [
    "print(generate('# <function name=\"test_http_404\">\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52aff0b-15fa-4feb-b731-200c17a62aa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
